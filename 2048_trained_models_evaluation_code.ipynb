{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Evaluation script for 2048 LLM agents.\n",
    "\n",
    "Plays multiple games using specified LLM configurations (local vLLM or cloud OpenAI API)\n",
    "and logs performance statistics (max tile, score, win rate) to a CSV file.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add these imports for visualization ---\n",
    "from IPython.display import display, clear_output, HTML, Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.font_manager as font_manager\n",
    "import io\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "import traceback\n",
    "\n",
    "import torch\n",
    "\n",
    "# VLLM Imports (only if vllm is installed and used)\n",
    "try:\n",
    "    from vllm import LLM, SamplingParams\n",
    "    VLLM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    VLLM_AVAILABLE = False\n",
    "    print(\"Warning: vLLM not found. Local model evaluation will be unavailable.\")\n",
    "\n",
    "# OpenAI Imports (only if openai library is installed and used)\n",
    "try:\n",
    "    from openai import OpenAI, APIError\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "    print(\"Warning: OpenAI library not found. Cloud model evaluation will be unavailable.\")\n",
    "\n",
    "print(f\"vLLM Available: {VLLM_AVAILABLE}\")\n",
    "print(f\"OpenAI Available: {OPENAI_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "\n",
    "# 1. Model Configurations (Add your models here)\n",
    "MODEL_CONFIGS = [\n",
    "    # Example Local vLLM Model\n",
    "    {\n",
    "        \"name\": \"Qwen/Qwen2.5-7B-Instruct_2048_game_grpo_merged_lora_9_simple_density_reward_2048_game_difficulty_1_2_3_4_5_ds2000_model_Qwen2.5-7B-Instruct_lr_4e-05_lora_rank_16/\",\n",
    "        \"type\": \"vllm\",\n",
    "        \"path\": \"Qwen/Qwen2.5-7B-Instruct_2048_game_grpo_merged_lora_9_simple_density_reward_2048_game_difficulty_1_2_3_4_5_ds2000_model_Qwen2.5-7B-Instruct_lr_4e-05_lora_rank_16/\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": 'Qwen/Qwen2.5-7B-Instruct_2048_game_grpo_merged_lora_10_simple_density_reward_2048_game_difficulty_1_2_3_4_5_ds8000_model_Qwen2.5-7B-Instruct_lr_4e-05_lora_rank_16',\n",
    "        'type': 'vllm',\n",
    "        'path': 'Qwen/Qwen2.5-7B-Instruct_2048_game_grpo_merged_lora_10_simple_density_reward_2048_game_difficulty_1_2_3_4_5_ds8000_model_Qwen2.5-7B-Instruct_lr_4e-05_lora_rank_16'\n",
    "    }\n",
    "    # Example OpenAI-Compatible API Model (e.g., Together.ai, Anyscale, local Llama.cpp server)\n",
    "    # {\n",
    "    #     \"name\": \"Mistral-7B-Instruct-v0.2-Cloud\",\n",
    "    #     \"type\": \"openai\",\n",
    "    #     \"api_model_name\": \"mistralai/Mistral-7B-Instruct-v0.2\", # Model name used by the API\n",
    "    #     \"base_url\": \"https://api.together.xyz/v1\",             # *** CHANGE THIS *** API endpoint URL\n",
    "    #     \"api_key\": \"YOUR_TOGETHER_API_KEY\",                   # *** CHANGE THIS *** Or set as ENV variable OPENAI_API_KEY\n",
    "    # },\n",
    "    # Example Official OpenAI Model\n",
    "    # {\n",
    "    #     \"name\": \"GPT-4o-Mini\",\n",
    "    #     \"type\": \"openai\",\n",
    "    #     \"api_model_name\": \"gpt-4o-mini\",\n",
    "    #     \"base_url\": \"https://api.openai.com/v1\",              # Official OpenAI endpoint\n",
    "    #     \"api_key\": \"YOUR_OPENAI_API_KEY\",                     # *** CHANGE THIS *** Or set as ENV variable OPENAI_API_KEY\n",
    "    # },\n",
    "]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Generation Configuration\n",
    "GENERATION_CONFIG = {\n",
    "    \"temperature\": 1.0,\n",
    "    \"max_new_tokens\": 1024, # Max tokens for the LLM's response (thinking + answer)\n",
    "    # Add other SamplingParams for vLLM or OpenAI params as needed (e.g., top_p, top_k)\n",
    "    # \"top_p\": 0.9,\n",
    "    # \"min_p\": 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Style Configurations (Copied from your training code) ---\n",
    "VLM_STYLES = [\n",
    "    {   # Style 1: Classic 2048\n",
    "        'bg_color': '#faf8ef',\n",
    "        'cell_color': ['#eee4da', '#ede0c8', '#f2b179', '#f59563', '#f67c5f', '#f65e3b', '#edcf72', '#edcc61', '#edc850', '#edc53f', '#edc22e'], # Extended colors for higher tiles\n",
    "        'text_color': '#776e65',\n",
    "        'grid_color': '#bbada0', # Adjusted grid color slightly\n",
    "        'font': 'Clear Sans' # Make sure this font is available or use a default like 'DejaVu Sans'\n",
    "    },\n",
    "    {   # Style 2: Dark Mode\n",
    "        'bg_color': '#1a1a1a', # Darker background\n",
    "        'cell_color': ['#3c3a32', '#5e5a51', '#7c726a', '#9b8a7e', '#b9a291', '#d7ba9e', '#f5d3ab', '#f8e0b4', '#fceecb', '#fff6e2', '#fffaf0'], # Gradient from dark to light beige/gold\n",
    "        'text_color': '#f9f6f2', # Off-white text\n",
    "        'grid_color': '#504b43', # Dark grid lines\n",
    "        'font': 'DejaVu Sans' # Common default font\n",
    "    },\n",
    "    # Add your other styles (Neon, Pastel) here if desired\n",
    "]\n",
    "# --- Font Check ---\n",
    "# Check if the default font exists, otherwise fallback\n",
    "default_font_name = 'Clear Sans' # Or your preferred default\n",
    "try:\n",
    "    font_manager.findfont(default_font_name)\n",
    "    print(f\"Default font '{default_font_name}' found.\")\n",
    "except:\n",
    "    print(f\"Warning: Default font '{default_font_name}' not found. Falling back to 'DejaVu Sans'.\")\n",
    "    default_font_name = 'DejaVu Sans'\n",
    "    VLM_STYLES[0]['font'] = default_font_name # Update style 1 if needed\n",
    "\n",
    "# --- Visualization Config ---\n",
    "VISUALIZE_GAME = True # Set to True to visualize the first game of the first model\n",
    "VISUALIZATION_DELAY_SECONDS = 0.75 # Pause between visualized steps\n",
    "VIZ_STYLE = VLM_STYLES[0] # Choose a style (e.g., the first one)\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... other Evaluation Run Config ...\n",
    "MAX_GAME_MOVES = 2000 # Safety break for games that might run too long\n",
    "\n",
    "# --- Add Max Restarts Config ---\n",
    "MAX_RESTARTS_PER_RUN = 3 # Max times to restart a single game run if agent gets stuck\n",
    "# ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_matrix_vlm_display(board, style=None):\n",
    "    \"\"\"Generates styled visualization of the board as base64 PNG for display.\"\"\"\n",
    "    style = style or random.choice(VLM_STYLES)\n",
    "    font_name = style.get('font', 'DejaVu Sans') # Use default if font missing\n",
    "\n",
    "    # Check if specified font exists, fallback if not\n",
    "    try:\n",
    "        font_path = font_manager.findfont(font_name)\n",
    "        font_prop = font_manager.FontProperties(fname=font_path)\n",
    "    except:\n",
    "        print(f\"Warning: Font '{font_name}' not found. Using 'DejaVu Sans'.\")\n",
    "        font_prop = font_manager.FontProperties(fname=font_manager.findfont('DejaVu Sans'))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 4)) # Slightly smaller figure for display\n",
    "    ax.set_facecolor(style['bg_color'])\n",
    "\n",
    "    # --- Define a color mapping for tile values ---\n",
    "    # Create a dictionary mapping power-of-2 values to colors\n",
    "    max_power = 12 # Up to 4096 (2^12), adjust if needed\n",
    "    tile_values = [2**i for i in range(1, max_power + 1)]\n",
    "    # Ensure enough colors are defined in the style, repeat last color if needed\n",
    "    num_colors_needed = len(tile_values)\n",
    "    style_colors = style['cell_color']\n",
    "    if len(style_colors) < num_colors_needed:\n",
    "        style_colors.extend([style_colors[-1]] * (num_colors_needed - len(style_colors)))\n",
    "\n",
    "    color_map = {val: style_colors[i] for i, val in enumerate(tile_values)}\n",
    "    default_cell_color = style_colors[0] # Color for empty or unexpected values\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    # --- Determine text color based on tile value ---\n",
    "    # Simple heuristic: lighter text for darker tiles, darker text for lighter tiles\n",
    "    # (You might need a more sophisticated approach depending on your color palettes)\n",
    "    light_text_color = '#f9f6f2' # Example light color\n",
    "    dark_text_color = style.get('text_color', '#776e65') # Default dark from style\n",
    "\n",
    "    # Plot cells\n",
    "    for r in range(4):\n",
    "        for c in range(4):\n",
    "            value = board[r][c]\n",
    "            cell_color = color_map.get(value, default_cell_color) # Get color from map\n",
    "\n",
    "            # Draw cell\n",
    "            rect = plt.Rectangle((c, 3-r), 1, 1,\n",
    "                               facecolor=cell_color,\n",
    "                               edgecolor=style['grid_color'],\n",
    "                               linewidth=2) # Thinner lines might look better\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Add text\n",
    "            if value > 0:\n",
    "                # Decide text color (simple check for higher values)\n",
    "                text_color = dark_text_color if value <= 4 else light_text_color\n",
    "\n",
    "                ax.text(c + 0.5, 3.5 - r, str(value),\n",
    "                        color=text_color,\n",
    "                        fontproperties=font_prop,\n",
    "                        ha='center', va='center',\n",
    "                        fontsize=16 if value < 1000 else 14, # Adjust font size slightly for large numbers\n",
    "                        weight='bold')\n",
    "\n",
    "    # Configure axes\n",
    "    ax.set_xlim(0, 4)\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect('equal')\n",
    "    plt.tight_layout(pad=0.1) # Reduce padding\n",
    "\n",
    "    # Save plot to a BytesIO buffer\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', bbox_inches='tight', dpi=90) # Lower DPI for faster display\n",
    "    plt.close(fig) # Close the figure to free memory\n",
    "    buf.seek(0)\n",
    "\n",
    "    # Encode buffer to base64\n",
    "    img_str = base64.b64encode(buf.read()).decode('utf-8')\n",
    "    return f\"data:image/png;base64,{img_str}\"\n",
    "\n",
    "print(\"Visualization function 'format_matrix_vlm_display' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a deep thinker. You analyze problems and provide answers as asked of you. When answering you provide your reasoning in <thinking> tags and final answer in <answer> tags. <think>. Write your detailed analysis and step-by-step reasoning here </think> <answer>Write your final response here </answer>. You do not say anything outside of the tags. You follow the format exactly as asked of you.\n",
    "\n",
    "Your ideal response should be in the following format:\n",
    "<think>\n",
    "[Detailed thinking]\n",
    "</think>\n",
    "<answer>\n",
    "Final answer\n",
    "</answer>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_prompt = \"\"\"You are an expert 2048 game player. Given a board state, select the best next move.\n",
    "\n",
    "The 2048 game board is a 4x4 grid where tiles with powers of 2 can be merged by swiping.\n",
    "When identical tiles collide, they combine into one tile with double the value.\n",
    "\n",
    "Current board state:\n",
    "{board}\n",
    "\n",
    "Analyze this board carefully. Consider:\n",
    "1. Available empty cells\n",
    "2. Potential tile merges\n",
    "3. High-value tile positions\n",
    "4. Avoiding game-ending situations\n",
    "\n",
    "Based on your analysis, what is the BEST move? Choose only one direction:\n",
    "- up\n",
    "- down\n",
    "- left\n",
    "- right\n",
    "\n",
    "Format your response as follows:\n",
    "<think>\n",
    "Analyze the current board configuration thoroughly. Consider what happens with each possible move (up, down, left, right). Think about immediate merges, resulting empty cells, and how the board position affects future possibilities. Consider how each move impacts your ability to create higher-value tiles and maximize score. Think several moves ahead if possible. Then make your decision to move up, down, left, or right.\n",
    "</think>\n",
    "<answer>\n",
    "[SINGLE WORD RESPONSE: up/down/left/right]\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Evaluation Run Configuration\n",
    "NUM_RUNS = 20 # Number of games to play per model configuration\n",
    "RESULTS_FILE = \"2048_evaluation_results.csv\" # Persistent CSV log file\n",
    "MAX_GAME_MOVES = 2000 # Safety break for games that might run too long\n",
    "\n",
    "# 4. vLLM Specific Configuration (if using vLLM)\n",
    "VLLM_GPU_MEMORY_UTILIZATION = 0.90\n",
    "VLLM_TENSOR_PARALLEL_SIZE = 1 # Adjust if using multiple GPUs\n",
    "VLLM_DEVICE = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 1: Core Game Logic (Stable Version - Mostly Unchanged) ---\n",
    "\n",
    "def initialize_board(size=4):\n",
    "    return np.zeros((size, size), dtype=int)\n",
    "\n",
    "# *** MODIFIED add_new_tile ***\n",
    "def add_new_tile(board, rng_random):\n",
    "    \"\"\"Adds a 2 (90%) or 4 (10%) to a random empty cell. Uses provided RNG.\"\"\"\n",
    "    empty_cells = np.argwhere(board == 0)\n",
    "    num_empty = len(empty_cells) # Get length explicitly\n",
    "    if num_empty == 0:\n",
    "        return False\n",
    "    # Use randint to get an index from 0 to num_empty - 1\n",
    "    random_index = rng_random.randint(0, num_empty - 1)\n",
    "    y, x = empty_cells[random_index]\n",
    "    # Use the worker's random instance for value\n",
    "    value = 4 if rng_random.random() < 0.1 else 2\n",
    "    board[y, x] = value\n",
    "    return True\n",
    "\n",
    "def _slide_row_left(row):\n",
    "    # (Implementation remains the same as previous correct version)\n",
    "    new_row = np.zeros_like(row); write_idx = 0; score_delta = 0\n",
    "    merged_in_row = [False] * len(row)\n",
    "    compacted_row = row[row != 0]; read_idx = 0\n",
    "    while read_idx < len(compacted_row):\n",
    "        val = compacted_row[read_idx]\n",
    "        if read_idx + 1 < len(compacted_row) and val == compacted_row[read_idx + 1] and not merged_in_row[write_idx]:\n",
    "             merged_val = val * 2; new_row[write_idx] = merged_val\n",
    "             score_delta += merged_val; merged_in_row[write_idx] = True\n",
    "             write_idx += 1; read_idx += 2\n",
    "        else:\n",
    "             new_row[write_idx] = val; write_idx += 1; read_idx += 1\n",
    "    return new_row, score_delta\n",
    "\n",
    "\n",
    "def move(board, direction):\n",
    "    # (Implementation remains the same as previous correct version)\n",
    "    size = board.shape[0]; original_board = board; total_score_delta = 0\n",
    "    rotations = {'up': 1, 'right': 2, 'down': 3, 'left': 0}.get(direction, 0)\n",
    "    if rotations > 0: rotated_board = np.rot90(original_board, k=rotations)\n",
    "    else: rotated_board = original_board\n",
    "    processed_board = np.zeros_like(rotated_board); changed_in_any_row = False\n",
    "    for i in range(size):\n",
    "        row = rotated_board[i, :]; new_row, score_delta = _slide_row_left(row)\n",
    "        processed_board[i, :] = new_row; total_score_delta += score_delta\n",
    "        if not np.array_equal(row, new_row): changed_in_any_row = True\n",
    "    if rotations > 0: final_board = np.rot90(processed_board, k=-rotations)\n",
    "    else: final_board = processed_board\n",
    "    changed = not np.array_equal(original_board, final_board)\n",
    "    return final_board, changed, total_score_delta\n",
    "\n",
    "def get_valid_moves(board):\n",
    "    # (Implementation remains the same as previous correct version)\n",
    "    valid = []\n",
    "    for direction in ['up', 'down', 'left', 'right']:\n",
    "        _, changed, _ = move(board, direction)\n",
    "        if changed: valid.append(direction)\n",
    "    return valid\n",
    "\n",
    "def is_game_over(board):\n",
    "    # (Implementation remains the same as previous correct version)\n",
    "    if np.any(board == 0): return False\n",
    "    return not get_valid_moves(board)\n",
    "\n",
    "print(\"Game logic functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a deep thinker. You analyze problems and provide answers as asked of you. When answering you provide your reasoning in <thinking> tags and final answer in <answer> tags. <think>. Write your detailed analysis and step-by-step reasoning here </think> <answer>Write your final response here </answer>. You do not say anything outside of the tags. You follow the format exactly as asked of you.\n",
    "\n",
    "Your ideal response should be in the following format:\n",
    "<think>\n",
    "[Detailed thinking]\n",
    "</think>\n",
    "<answer>\n",
    "Final answer\n",
    "</answer>\"\"\"\n",
    "\n",
    "\n",
    "llm_prompt_template = \"\"\"You are an expert 2048 game player. Given a board state, select the best next move.\n",
    "\n",
    "The 2048 game board is a 4x4 grid where tiles with powers of 2 can be merged by swiping.\n",
    "When identical tiles collide, they combine into one tile with double the value.\n",
    "\n",
    "Current board state:\n",
    "{board_text}\n",
    "\n",
    "Analyze this board carefully. Consider:\n",
    "1. Available empty cells\n",
    "2. Potential tile merges\n",
    "3. High-value tile positions\n",
    "4. Avoiding game-ending situations\n",
    "\n",
    "Based on your analysis, what is the BEST move? Choose only one direction:\n",
    "- up\n",
    "- down\n",
    "- left\n",
    "- right\n",
    "\n",
    "Format your response as follows:\n",
    "<think>\n",
    "Analyze the current board configuration thoroughly. Consider what happens with each possible move (up, down, left, right). Think about immediate merges, resulting empty cells, and how the board position affects future possibilities. Consider how each move impacts your ability to create higher-value tiles and maximize score. Think several moves ahead if possible. Then make your decision to move up, down, left, or right.\n",
    "</think>\n",
    "<answer>\n",
    "[SINGLE WORD RESPONSE: up/down/left/right]\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_matrix_llm(board, zero_char='.'):\n",
    "    \"\"\"Formats game board into a simple text representation for the LLM prompt.\"\"\"\n",
    "    size = len(board)\n",
    "    max_len = len(str(np.max(board))) if np.any(board > 0) else 1\n",
    "\n",
    "    lines = []\n",
    "    for row in board:\n",
    "        cells = []\n",
    "        for x in row:\n",
    "            if x == 0:\n",
    "                cells.append(zero_char.rjust(max_len))\n",
    "            else:\n",
    "                cells.append(str(x).rjust(max_len))\n",
    "        lines.append(\" | \".join(cells))\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_action(completion: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extracts the action from the <answer> tag in the LLM completion.\n",
    "    Returns lowercase action ('up', 'down', 'left', 'right') or None if invalid/missing.\n",
    "    \"\"\"\n",
    "    if not completion:\n",
    "        return None\n",
    "    match = re.search(r\"<answer>\\s*(.*?)\\s*</answer>\", completion, re.IGNORECASE | re.DOTALL)\n",
    "    if not match:\n",
    "        # Fallback: Check if the *entire* response is just the action word\n",
    "        action_simple = completion.strip().lower()\n",
    "        if action_simple in {\"up\", \"down\", \"left\", \"right\"}:\n",
    "             print(f\"Warning: Could not parse <answer> tag, but found valid action '{action_simple}' as full response.\")\n",
    "             return action_simple\n",
    "        # print(f\"Warning: Could not parse <answer> tag in completion: '{completion[:100]}...'\")\n",
    "        return None\n",
    "\n",
    "    action = match.group(1).strip().lower()\n",
    "    if action in {\"up\", \"down\", \"left\", \"right\"}:\n",
    "        return action\n",
    "    else:\n",
    "        # print(f\"Warning: Invalid action '{action}' found in <answer> tag.\")\n",
    "        return None\n",
    "\n",
    "print(\"Prompting and parsing functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sampling_params(generation_config: Dict) -> SamplingParams:\n",
    "    \"\"\"Creates vLLM SamplingParams from generation config dictionary.\"\"\"\n",
    "    \n",
    "    # Convert all possible parameters from generation_config to sampling_kwargs\n",
    "    sampling_kwargs = {}\n",
    "    \n",
    "    # Basic parameters\n",
    "    if \"temperature\" in generation_config:\n",
    "        sampling_kwargs[\"temperature\"] = generation_config[\"temperature\"]\n",
    "    if \"max_new_tokens\" in generation_config:\n",
    "        sampling_kwargs[\"max_tokens\"] = generation_config[\"max_new_tokens\"]\n",
    "    if \"top_p\" in generation_config:\n",
    "        sampling_kwargs[\"top_p\"] = generation_config[\"top_p\"]\n",
    "    if \"top_k\" in generation_config:\n",
    "        sampling_kwargs[\"top_k\"] = generation_config[\"top_k\"]\n",
    "    \n",
    "    # Additional parameters\n",
    "    if \"min_p\" in generation_config:\n",
    "        sampling_kwargs[\"min_p\"] = generation_config[\"min_p\"]\n",
    "    if \"presence_penalty\" in generation_config:\n",
    "        sampling_kwargs[\"presence_penalty\"] = generation_config[\"presence_penalty\"]\n",
    "    if \"frequency_penalty\" in generation_config:\n",
    "        sampling_kwargs[\"frequency_penalty\"] = generation_config[\"frequency_penalty\"]\n",
    "    if \"repetition_penalty\" in generation_config:\n",
    "        sampling_kwargs[\"repetition_penalty\"] = generation_config[\"repetition_penalty\"]\n",
    "    \n",
    "    # Stop sequences\n",
    "    if \"stop\" in generation_config:\n",
    "        sampling_kwargs[\"stop\"] = generation_config[\"stop\"]\n",
    "    \n",
    "    # Create SamplingParams with only the parameters that were specified\n",
    "    return SamplingParams(**sampling_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 3: LLM Inference ---\n",
    "\n",
    "class LLMInference:\n",
    "    \"\"\"Base class for LLM inference engines.\"\"\"\n",
    "    def __init__(self, model_config: Dict):\n",
    "        self.model_config = model_config\n",
    "        self.model_name = model_config[\"name\"]\n",
    "\n",
    "    def generate(self, prompt_messages: List[Dict], generation_config: Dict) -> Tuple[Optional[str], Optional[str]]:\n",
    "        \"\"\"Generates a response given prompt messages. Returns (completion, error_message).\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Releases any resources (e.g., GPU memory).\"\"\"\n",
    "        pass\n",
    "\n",
    "# Import torch needed for cleanup cache clearing\n",
    "# Ensure 'torch', 'os', 'traceback' are imported earlier in the notebook\n",
    "# import torch\n",
    "# import os\n",
    "# import traceback\n",
    "\n",
    "class VLLMInference(LLMInference):\n",
    "    \"\"\"Inference using local vLLM.\"\"\"\n",
    "    def __init__(self, model_config: Dict):\n",
    "        super().__init__(model_config)\n",
    "        if not VLLM_AVAILABLE:\n",
    "            raise RuntimeError(\"vLLM is not installed, cannot use VLLMInference.\")\n",
    "\n",
    "        # --- Determine Device ID from VLLM_DEVICE ---\n",
    "        # Assuming VLLM_DEVICE is like \"cuda:1\"\n",
    "        try:\n",
    "            if isinstance(VLLM_DEVICE, str) and VLLM_DEVICE.startswith(\"cuda:\"):\n",
    "                 device_id_str = VLLM_DEVICE.split(\":\")[-1]\n",
    "                 # This specific ID will be made visible via CUDA_VISIBLE_DEVICES\n",
    "                 target_device_id_list = [int(device_id_str)]\n",
    "                 # Since we target one specific device, tensor_parallel_size must be 1\n",
    "                 if VLLM_TENSOR_PARALLEL_SIZE != 1:\n",
    "                     print(f\"Warning: VLLM_DEVICE ('{VLLM_DEVICE}') implies a single device, overriding VLLM_TENSOR_PARALLEL_SIZE to 1.\")\n",
    "                 tensor_parallel_size = 1\n",
    "            # Example for handling multiple devices like \"cuda:1,\n",
    "            else:\n",
    "                # Fallback or handle error if VLLM_DEVICE format is unexpected\n",
    "                print(f\"Warning: VLLM_DEVICE ('{VLLM_DEVICE}') format not recognized or not set. Relying on default CUDA visibility and VLLM_TENSOR_PARALLEL_SIZE={VLLM_TENSOR_PARALLEL_SIZE}.\")\n",
    "                target_device_id_list = None # Let vLLM use default visibility\n",
    "                tensor_parallel_size = VLLM_TENSOR_PARALLEL_SIZE\n",
    "\n",
    "        except ValueError:\n",
    "             print(f\"Error parsing VLLM_DEVICE ('{VLLM_DEVICE}'). Using default visibility.\")\n",
    "             target_device_id_list = None\n",
    "             tensor_parallel_size = VLLM_TENSOR_PARALLEL_SIZE\n",
    "        # ---------------------------------------------\n",
    "\n",
    "        print(f\"Initializing vLLM for model: {model_config['path']}...\")\n",
    "        print(f\"  Attempting target devices: {target_device_id_list}, tensor_parallel_size: {tensor_parallel_size}\")\n",
    "\n",
    "        original_cuda_visible = os.environ.get(\"CUDA_VISIBLE_DEVICES\")\n",
    "        if target_device_id_list is not None:\n",
    "            # Attempt to set CUDA_VISIBLE_DEVICES for this specific initialization\n",
    "            print(f\"  Temporarily setting CUDA_VISIBLE_DEVICES='{','.join(map(str, target_device_id_list))}'\")\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, target_device_id_list))\n",
    "        else:\n",
    "            # If no specific device ID was parsed, don't modify the environment var\n",
    "            print(\"  Using existing CUDA_VISIBLE_DEVICES environment setting (if any).\")\n",
    "\n",
    "        try:\n",
    "            self.llm = LLM(\n",
    "                model=model_config[\"path\"],\n",
    "                tensor_parallel_size=tensor_parallel_size, # Use determined size\n",
    "                gpu_memory_utilization=VLLM_GPU_MEMORY_UTILIZATION,\n",
    "                trust_remote_code=True, # Often needed for \n",
    "            )\n",
    "            self.tokenizer = self.llm.get_tokenizer()\n",
    "\n",
    "            \n",
    "            print(\"vLLM initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vLLM: {e}\")\n",
    "            traceback.print_exc()\n",
    "            raise # Re-raise the exception after printing\n",
    "        finally:\n",
    "            # --- Restore original CUDA_VISIBLE_DEVICES ---\n",
    "            if original_cuda_visible is None:\n",
    "                # If it didn't exist before, remove it\n",
    "                if 'CUDA_VISIBLE_DEVICES' in os.environ: del os.environ['CUDA_VISIBLE_DEVICES']\n",
    "                # print(\"  Restored CUDA_VISIBLE_DEVICES (removed).\")\n",
    "            else:\n",
    "                # Otherwise, restore the original value\n",
    "                os.environ['CUDA_VISIBLE_DEVICES'] = original_cuda_visible\n",
    "                # print(f\"  Restored CUDA_VISIBLE_DEVICES to '{original_cuda_visible}'.\")\n",
    "            # ---------------------------------------------\n",
    "\n",
    "    def generate(self, prompt_messages: List[Dict], generation_config: Dict) -> Tuple[Optional[str], Optional[str]]:\n",
    "        \"\"\"Generates a response given prompt messages using vLLM.\"\"\"\n",
    "        try:\n",
    "            sampling_params = create_sampling_params(generation_config)\n",
    "            # Apply chat template (important for instruction-tuned models)\n",
    "            # Note: vLLM expects a single string prompt after applying the template\n",
    "            prompt_str = self.tokenizer.apply_chat_template(\n",
    "                prompt_messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True # Ensures the model knows to generate the next turn\n",
    "            )\n",
    "\n",
    "            outputs = self.llm.generate(prompt_str, sampling_params, use_tqdm=False)\n",
    "            # vLLM returns a list of RequestOutput objects\n",
    "            if outputs and outputs[0].outputs:\n",
    "                completion = outputs[0].outputs[0].text\n",
    "                return completion, None\n",
    "            else:\n",
    "                return None, \"vLLM generation returned no output.\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error during vLLM generation: {e}\")\n",
    "            # traceback.print_exc() # Uncomment for detailed stack trace if needed\n",
    "            return None, str(e)\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Releases vLLM resources.\"\"\"\n",
    "        if hasattr(self, 'llm'):\n",
    "            # Explicitly delete the model and clear cache if possible\n",
    "            # This might be necessary depending on the vLLM version and setup\n",
    "            # to fully release GPU memory before loading the next model.\n",
    "            print(f\"Releasing vLLM resources for {self.model_name}...\")\n",
    "            del self.llm\n",
    "            del self.tokenizer\n",
    "            # Attempt to clear CUDA cache if torch is available\n",
    "            if 'torch' in globals() and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            print(\"vLLM resources released.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "VLLM_GPU_MEMORY_UTILIZATION = 0.90\n",
    "# VLLM_TENSOR_PARALLEL_SIZE = 1 # Adjust if using multiple GPUs <-- CHANGE THIS\n",
    "VLLM_TENSOR_PARALLEL_SIZE = 2 # Use 2 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Test Initialization Cell ---\n",
    "\n",
    "# # Set VLLM config for GPUs 1 and 2\n",
    "# VLLM_TENSOR_PARALLEL_SIZE = 2\n",
    "# VLLM_DEVICE = \"cuda:1,2\" # Relies on external CUDA_VISIBLE_DEVICES=1,2\n",
    "\n",
    "# print(\"--- Testing VLLMInference Initialization ---\")\n",
    "# test_llm_player = None\n",
    "# try:\n",
    "#     test_model_config = MODEL_CONFIGS[0]\n",
    "#     if test_model_config[\"type\"] == \"vllm\" and VLLM_AVAILABLE:\n",
    "#         print(f\"Attempting to initialize {test_model_config['name']}...\")\n",
    "#         # Assumes CUDA_VISIBLE_DEVICES=1,2 is set externally before launch\n",
    "#         test_llm_player = VLLMInference(test_model_config, GENERATION_CONFIG)\n",
    "#         print(\"--- VLLMInference Initialization Test Successful ---\")\n",
    "#     else:\n",
    "#         print(f\"Skipping test: Model type is '{test_model_config.get('type', 'N/A')}' or vLLM available is {VLLM_AVAILABLE}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(\"\\n--- VLLMInference Initialization Test FAILED ---\")\n",
    "#     print(f\"Error: {type(e).__name__} - {e}\")\n",
    "#     traceback.print_exc()\n",
    "# finally:\n",
    "#     if test_llm_player is not None:\n",
    "#         print(\"Cleaning up test player...\")\n",
    "#         test_llm_player.cleanup()\n",
    "\n",
    "# print(\"\\n--- Initialization Test Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 4: Evaluation Loop ---\n",
    "\n",
    "# (Keep format_matrix_vlm_display function as defined previously)\n",
    "\n",
    "def play_game(\n",
    "    llm_inference: LLMInference,\n",
    "    generation_config: Dict,\n",
    "    game_seed: Optional[int] = None,\n",
    "    visualize: bool = False,\n",
    "    viz_delay: float = 1.0,\n",
    "    viz_style: Optional[Dict] = None,\n",
    "    move_no_matter_what: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Plays a single game of 2048 using the provided LLM.\n",
    "    Restarts the game if the agent gets stuck, up to MAX_RESTARTS_PER_RUN.\n",
    "    Reports stats from the final attempt, plus overall bests and restart count.\n",
    "    Can optionally visualize steps graphically in Jupyter.\n",
    "    \"\"\"\n",
    "    if game_seed is None: game_seed = random.randint(0, 2**32 - 1)\n",
    "        # Disable clear_output for debugging\n",
    "    def debug_display(*args, **kwargs):\n",
    "        if 'clear_output' in kwargs:\n",
    "            del kwargs['clear_output']\n",
    "        display(*args, **kwargs)\n",
    "    \n",
    "    # Use debug_display instead of display for visualization\n",
    "    if visualize:\n",
    "        display_func = debug_display\n",
    "    else:\n",
    "        display_func = display\n",
    "    total_restarts_done = 0\n",
    "    overall_max_tile_across_restarts = 0\n",
    "    overall_max_score_across_restarts = 0\n",
    "\n",
    "    # --- Loop for restarts ---\n",
    "    while total_restarts_done <= MAX_RESTARTS_PER_RUN:\n",
    "        attempt_seed = game_seed + total_restarts_done # Vary seed for each attempt\n",
    "        game_rng = random.Random(attempt_seed)\n",
    "\n",
    "        if visualize and total_restarts_done > 0:\n",
    "             clear_output(wait=True)\n",
    "             display(HTML(f\"<h2>Attempt {total_restarts_done + 1} / {MAX_RESTARTS_PER_RUN + 1} (Seed: {attempt_seed})</h2>\"))\n",
    "             time.sleep(viz_delay * 1.5)\n",
    "\n",
    "        # --- Initialize state for this attempt ---\n",
    "        board = initialize_board()\n",
    "        add_new_tile(board, game_rng)\n",
    "        add_new_tile(board, game_rng)\n",
    "        score = 0\n",
    "        moves_made = 0\n",
    "        max_tile_this_attempt = np.max(board)\n",
    "        last_error_this_attempt = None # Error specific to this attempt\n",
    "        stuck_counter = 0\n",
    "        start_time_attempt = time.time() # Track duration of the last attempt mostly\n",
    "        # ----------------------------------------\n",
    "\n",
    "        # --- Inner game loop for this attempt ---\n",
    "        while moves_made < MAX_GAME_MOVES:\n",
    "            pre_move_board = board.copy()\n",
    "            current_max = np.max(board)\n",
    "            if current_max > max_tile_this_attempt: max_tile_this_attempt = current_max\n",
    "\n",
    "            # --- Check Game Over ---\n",
    "            if is_game_over(board):\n",
    "                last_error_this_attempt = None # Natural end\n",
    "                if visualize:\n",
    "                    clear_output(wait=True)\n",
    "                    board_img_data = format_matrix_vlm_display(board, style=viz_style)\n",
    "                    html_content = f\"<h2>Game Over! (Attempt {total_restarts_done+1})</h2>\"\n",
    "                    html_content += f\"<p>Move: {moves_made}, Score: {score}, Max Tile: {current_max}</p>\"\n",
    "                    html_content += f\"<img src='{board_img_data}' alt='Final Board State' style='width: 250px; height: auto;'/>\"\n",
    "                    display(HTML(html_content))\n",
    "                    time.sleep(viz_delay * 2)\n",
    "                break # Exit inner loop for this attempt\n",
    "\n",
    "            valid_moves = get_valid_moves(board)\n",
    "            if not valid_moves: # Should be caught by is_game_over\n",
    "                last_error_this_attempt = \"No valid moves left\"\n",
    "                if visualize: # Similar display as game over\n",
    "                     clear_output(wait=True)\n",
    "                     board_img_data = format_matrix_vlm_display(board, style=viz_style)\n",
    "                     html_content = f\"<h2>No Valid Moves! (Attempt {total_restarts_done+1})</h2>\"\n",
    "                     html_content += f\"<p>Move: {moves_made}, Score: {score}, Max Tile: {current_max}</p>\"\n",
    "                     html_content += f\"<img src='{board_img_data}' alt='Final Board State' style='width: 250px; height: auto;'/>\"\n",
    "                     display(HTML(html_content))\n",
    "                     time.sleep(viz_delay * 2)\n",
    "                break # Exit inner loop\n",
    "\n",
    "            # --- Prepare Prompt (no change) ---\n",
    "            board_text_representation = format_matrix_llm(board)\n",
    "            user_prompt = llm_prompt_template.format(board_text=board_text_representation)\n",
    "            messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "            # --- Display state before LLM call ---\n",
    "            if visualize:\n",
    "                clear_output(wait=True)\n",
    "                board_img_data = format_matrix_vlm_display(pre_move_board, style=viz_style)\n",
    "                html_content = f\"<h2>Attempt {total_restarts_done+1} / Move: {moves_made + 1} / Score: {score}</h2>\"\n",
    "                html_content += f\"<p>Board state before move:</p>\"\n",
    "                html_content += f\"<img src='{board_img_data}' alt='Board State' style='width: 250px; height: auto;'/>\"\n",
    "                html_content += f\"<p><i>Waiting for LLM ({llm_inference.model_name})...</i></p>\"\n",
    "                display(HTML(html_content))\n",
    "\n",
    "            # --- Get LLM Move ---\n",
    "            llm_response, error_msg = llm_inference.generate(messages, generation_config)\n",
    "\n",
    "            print(f\"llm prompt: {user_prompt}\")\n",
    "            print(f\"llm response: {llm_response}\")\n",
    "            if error_msg: # Critical LLM error\n",
    "                print(f\"  LLM Generation Error: {error_msg}. Ending game run.\")\n",
    "                last_error_this_attempt = f\"LLM Error: {error_msg}\"\n",
    "                if visualize:\n",
    "                    clear_output(wait=True)\n",
    "                    board_img_data = format_matrix_vlm_display(pre_move_board, style=viz_style)\n",
    "                    html_content = f\"<h2>Attempt {total_restarts_done+1} / Move: {moves_made + 1} / Score: {score}</h2>\"\n",
    "                    html_content += f\"<p>Board state (before error):</p>\"\n",
    "                    html_content += f\"<img src='{board_img_data}' alt='Board State Before Error' style='width: 250px; height: auto;'/>\"\n",
    "                    html_content += f\"<p style='color:red;'><b>LLM Error:</b> {error_msg}</p>\"\n",
    "                    display(HTML(html_content))\n",
    "                    time.sleep(viz_delay * 2)\n",
    "                break # Exit inner loop\n",
    "\n",
    "            action = parse_action(llm_response)\n",
    "\n",
    "            # --- Execute Move ---\n",
    "            final_board = None\n",
    "            viz_message = \"\"\n",
    "            action_taken_viz = \"N/A\"\n",
    "\n",
    "            if action:\n",
    "                action_taken_viz = action.upper()\n",
    "                new_board, changed, score_delta = move(board, action)\n",
    "                if changed:\n",
    "                    board = new_board\n",
    "                    add_new_tile(board, game_rng)\n",
    "                    score += score_delta\n",
    "                    moves_made += 1\n",
    "                    stuck_counter = 0\n",
    "                    final_board = board.copy()\n",
    "                    viz_message = f\"Action: <b>{action_taken_viz}</b> -> New Tile Added. Score +{score_delta}\"\n",
    "                else:\n",
    "                    stuck_counter += 1\n",
    "                    final_board = pre_move_board\n",
    "                    viz_message = f\"Action: <b>{action_taken_viz}</b> -> <span style='color:orange;'>No change!</span> (Stuck: {stuck_counter})\"\n",
    "                    if move_no_matter_what:\n",
    "                        print(\"DEBUG: Forcing tile add on no-change move.\")\n",
    "                        add_new_tile(board, game_rng)\n",
    "                        final_board = board.copy()\n",
    "                        viz_message += \" <span style='color:red;'>(Forced Tile Add)</span>\"\n",
    "            else:\n",
    "                stuck_counter += 1\n",
    "                action_taken_viz = \"Invalid/Unparsed\"\n",
    "                final_board = pre_move_board\n",
    "                viz_message = f\"Action: <span style='color:red;'>Invalid/Unparsed!</span> (Stuck: {stuck_counter})\"\n",
    "                if move_no_matter_what:\n",
    "                    print(\"DEBUG: Forcing tile add on invalid/unparsed action.\")\n",
    "                    add_new_tile(board, game_rng)\n",
    "                    final_board = board.copy()\n",
    "                    viz_message += \" <span style='color:red;'>(Forced Tile Add)</span>\"\n",
    "\n",
    "            # --- Visualization: Display step result ---\n",
    "            if visualize:\n",
    "                 clear_output(wait=True)\n",
    "                 pre_move_img_data = format_matrix_vlm_display(pre_move_board, style=viz_style)\n",
    "                 post_move_img_data = format_matrix_vlm_display(final_board, style=viz_style)\n",
    "                 html_content = f\"<h2>Attempt {total_restarts_done+1} / Move: {moves_made} / Score: {score}</h2>\" # Show completed moves for this attempt\n",
    "                 html_content += f\"<p>{viz_message}</p>\"\n",
    "                 html_content += \"<div style='display: flex; align-items: flex-start; gap: 20px;'>\"\n",
    "                 html_content += f\"<div>Before:<br/><img src='{pre_move_img_data}' alt='Board Before' style='width: 200px; height: auto;'/></div>\"\n",
    "                 html_content += f\"<div>After:<br/><img src='{post_move_img_data}' alt='Board After' style='width: 200px; height: auto;'/></div>\"\n",
    "                 html_content += \"</div>\"\n",
    "                 display(HTML(html_content))\n",
    "                 time.sleep(viz_delay)\n",
    "\n",
    "            # --- Check if stuck ---\n",
    "            if stuck_counter >= 5:\n",
    "                last_error_this_attempt = f\"Stuck after {stuck_counter} invalid moves\"\n",
    "                if visualize:\n",
    "                     clear_output(wait=True)\n",
    "                     board_img_data = format_matrix_vlm_display(final_board, style=viz_style)\n",
    "                     html_content = f\"<h2>Attempt {total_restarts_done+1} STUCK!</h2>\"\n",
    "                     html_content += f\"<p>Board state (when stuck):</p>\"\n",
    "                     html_content += f\"<img src='{board_img_data}' alt='Board State When Stuck' style='width: 250px; height: auto;'/>\"\n",
    "                     html_content += f\"<p style='color:red;'><b>LLM Stuck after {stuck_counter} invalid/no-op moves.</b></p>\"\n",
    "                     display(HTML(html_content))\n",
    "                     time.sleep(viz_delay * 1.5) # Pause longer on stuck\n",
    "                break # Exit inner loop to trigger restart or end run\n",
    "        # --- End of inner game loop for this attempt ---\n",
    "\n",
    "        # Update overall bests after the attempt finishes (stuck or game over)\n",
    "        overall_max_tile_across_restarts = max(overall_max_tile_across_restarts, max_tile_this_attempt)\n",
    "        overall_max_score_across_restarts = max(overall_max_score_across_restarts, score)\n",
    "\n",
    "        # --- Decide whether to restart or end the run ---\n",
    "        if last_error_this_attempt and \"Stuck\" in last_error_this_attempt:\n",
    "            total_restarts_done += 1\n",
    "            if total_restarts_done <= MAX_RESTARTS_PER_RUN:\n",
    "                print(f\"  Game Run (Seed {game_seed}): Attempt {total_restarts_done} failed (Stuck). Restarting attempt {total_restarts_done + 1}/{MAX_RESTARTS_PER_RUN + 1}...\")\n",
    "                continue # Continue the outer restart loop\n",
    "            else:\n",
    "                print(f\"  Game Run (Seed {game_seed}): Stuck on final attempt ({total_restarts_done}). Ending run.\")\n",
    "                break # Exit outer loop - max restarts reached\n",
    "        else:\n",
    "             # Game finished naturally (game over, no valid moves, or LLM error)\n",
    "             break # Exit outer loop\n",
    "    # --- End of outer restart loop ---\n",
    "\n",
    "    end_time = time.time() # Use overall end time\n",
    "    final_max_tile_last_attempt = np.max(board) # Max tile on the *very last* board\n",
    "    win_last_attempt = final_max_tile_last_attempt >= 2048\n",
    "\n",
    "    # --- Final Visualization Display (for the last attempt) ---\n",
    "    if visualize and not (last_error_this_attempt and \"Stuck\" in last_error_this_attempt): # Don't display final if we just showed 'Stuck' message\n",
    "         if last_error_this_attempt is None or \"No valid moves\" in last_error_this_attempt: # Check if it ended naturally or via no moves\n",
    "             clear_output(wait=True)\n",
    "             result_text = \"WON!\" if win_last_attempt else \"Game Over\"\n",
    "             board_img_data = format_matrix_vlm_display(board, style=viz_style)\n",
    "             html_content = f\"<h2>{result_text} (Attempt {total_restarts_done+1})</h2>\"\n",
    "             html_content += f\"<p>Final Score: {score}, Max Tile: {final_max_tile_last_attempt}, Moves: {moves_made}</p>\"\n",
    "             html_content += f\"<img src='{board_img_data}' alt='Final Board State' style='width: 250px; height: auto;'/>\"\n",
    "             display(HTML(html_content))\n",
    "             time.sleep(viz_delay*2) # Pause on final screen\n",
    "\n",
    "    # Return stats from the LAST attempt, plus overall bests and restart count\n",
    "    return {\n",
    "        \"model_name\": llm_inference.model_name,\n",
    "        \"inference_type\": llm_inference.model_config[\"type\"],\n",
    "        \"game_seed\": game_seed, # Original seed for the run\n",
    "        \"final_score\": score, # Score from the last attempt\n",
    "        \"max_tile_achieved\": int(final_max_tile_last_attempt), # Max tile on final board of last attempt\n",
    "        \"num_moves\": moves_made, # Moves in the last attempt\n",
    "        \"win\": win_last_attempt, # Win status based on last attempt's board\n",
    "        \"duration_seconds\": round(time.time() - start_time_attempt, 2), # Duration of the last attempt\n",
    "        \"temperature\": generation_config.get(\"temperature\"),\n",
    "        \"max_new_tokens\": generation_config.get(\"max_new_tokens\"),\n",
    "        \"top_p\": generation_config.get(\"top_p\"),\n",
    "        \"min_p\": generation_config.get(\"min_p\"),\n",
    "        # --- New fields ---\n",
    "        \"overall_max_tile_across_restarts\": int(overall_max_tile_across_restarts),\n",
    "        \"overall_max_score_across_restarts\": overall_max_score_across_restarts,\n",
    "        \"restarts_needed\": total_restarts_done,\n",
    "        # --- Final error ---\n",
    "        # Error reflects state at end of last attempt (e.g., None if natural end, 'Stuck...' if ended stuck)\n",
    "        \"error\": last_error_this_attempt if last_error_this_attempt else None,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 5: Data Storage ---\n",
    "\n",
    "def log_results(results: Dict[str, Any], filename: str):\n",
    "    \"\"\"Appends evaluation results to a CSV file.\"\"\"\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    # --- Define the desired order of columns --- ADD NEW FIELDS ---\n",
    "    fieldnames = [\n",
    "        \"timestamp\", \"model_name\", \"inference_type\", \"run_id\", \"game_seed\",\n",
    "        \"max_tile_achieved\", # From last attempt\n",
    "        \"final_score\",       # From last attempt\n",
    "        \"num_moves\",         # From last attempt\n",
    "        \"win\",               # From last attempt\n",
    "        \"overall_max_tile_across_restarts\", # New field\n",
    "        \"overall_max_score_across_restarts\",# New field\n",
    "        \"restarts_needed\",                  # New field\n",
    "        \"duration_seconds\",  # From last attempt\n",
    "        \"temperature\", \"max_new_tokens\", \"top_p\", \"min_p\", # Added min_p just in case\n",
    "        \"error\" # Error from last attempt (can indicate ended stuck)\n",
    "    ]\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            # Ensure all fieldnames exist in results before writing\n",
    "            for field in fieldnames:\n",
    "                results.setdefault(field, None) # Add missing keys with None value\n",
    "\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')\n",
    "\n",
    "            if not file_exists or os.path.getsize(filename) == 0:\n",
    "                writer.writeheader()\n",
    "\n",
    "            writer.writerow(results)\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing results to {filename}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during result logging: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# print(\"Data storage function updated.\") # Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_config = MODEL_CONFIGS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM with CUDA devices 1,2\n",
    "llm_player = VLLMInference(target_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": 'Introduce yourself as a 2048 expert and give a brief overview of the game.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"top_p\": 0.9,\n",
    "    \"min_p\": 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_player.generate(messages, generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== 2048 LLM Evaluation ===\\n\")\n",
    "\n",
    "# Take just the first model config\n",
    "model_config = MODEL_CONFIGS[0]\n",
    "model_name = model_config[\"name\"]\n",
    "\n",
    "try:\n",
    "    sleep(1)\n",
    "    \n",
    "    # Run ONE visualized game first\n",
    "    if VISUALIZE_GAME:\n",
    "        print(f\"\\n--- Running visualized game for {model_name} ---\")\n",
    "        game_result_viz = play_game(\n",
    "            llm_player,\n",
    "            GENERATION_CONFIG,\n",
    "            visualize=True,\n",
    "            viz_delay=VISUALIZATION_DELAY_SECONDS,\n",
    "            viz_style=VIZ_STYLE,\n",
    "            \n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- Visualization Game Summary ---\")\n",
    "        print(f\"Score: {game_result_viz.get('final_score', 'N/A')}\")\n",
    "        print(f\"Max Tile: {game_result_viz.get('max_tile_achieved', 'N/A')}\")\n",
    "        print(f\"Moves: {game_result_viz.get('num_moves', 'N/A')}\")\n",
    "        if game_result_viz.get('error'): \n",
    "            print(f\"Error: {game_result_viz['error']}\")\n",
    "    \n",
    "    # Run standard evaluation games\n",
    "    run_results = []\n",
    "    print(f\"\\nStarting {NUM_RUNS} evaluation runs...\")\n",
    "    for run_id in tqdm(range(1, NUM_RUNS + 1), desc=\"Playing games\", unit=\"game\"):\n",
    "        game_result = play_game(llm_player, GENERATION_CONFIG, visualize=False)\n",
    "        game_result[\"run_id\"] = run_id\n",
    "        log_results(game_result, RESULTS_FILE)\n",
    "        run_results.append(game_result)\n",
    "    \n",
    "    # Calculate stats\n",
    "    if run_results:\n",
    "        successful_runs = [r for r in run_results if not r.get('error')]\n",
    "        if successful_runs:\n",
    "            avg_score = np.mean([r['final_score'] for r in successful_runs])\n",
    "            avg_max_tile = np.mean([r['max_tile_achieved'] for r in successful_runs])\n",
    "            win_rate = np.mean([r['win'] for r in successful_runs]) * 100\n",
    "            \n",
    "            print(f\"\\nResults for {model_name}:\")\n",
    "            print(f\"Avg Score: {avg_score:.2f}\")\n",
    "            print(f\"Avg Max Tile: {avg_max_tile:.2f}\")\n",
    "            print(f\"Win Rate: {win_rate:.1f}%\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "# finally:\n",
    "#     if 'llm_player' in locals():\n",
    "#         llm_player.cleanup()\n",
    "\n",
    "print(f\"\\nResults logged to: {RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 6: Main Execution Block ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"=== 2048 LLM Evaluation ===\")\n",
    "print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "# Track overall statistics per model\n",
    "model_summary_stats = {}\n",
    "\n",
    "# --- Visualization Flag ---\n",
    "# Ensure VISUALIZE_GAME, VISUALIZATION_DELAY_SECONDS, VIZ_STYLE are defined in a previous cell\n",
    "run_visualization = VISUALIZE_GAME\n",
    "# --------------------------\n",
    "\n",
    "for config_idx, model_config in enumerate(MODEL_CONFIGS):\n",
    "    model_name = model_config[\"name\"]\n",
    "    inference_type = model_config[\"type\"]\n",
    "    print(f\"\\n--- Evaluating Model {config_idx+1}/{len(MODEL_CONFIGS)}: {model_name} ({inference_type}) ---\")\n",
    "\n",
    "    llm_player = None\n",
    "    try:\n",
    "        # --- Initialize LLM ---\n",
    "        # Ensure VLLM settings like VLLM_TENSOR_PARALLEL_SIZE and VLLM_DEVICE\n",
    "        # are defined in a previous cell according to your target setup.\n",
    "        # **Crucially, set CUDA_VISIBLE_DEVICES environment variable *before* launching\n",
    "        # Jupyter/Python for specific device targeting.**\n",
    "        # Example for GPUs 1 & 2: export CUDA_VISIBLE_DEVICES=1,2\n",
    "        if inference_type == \"vllm\":\n",
    "            if not VLLM_AVAILABLE:\n",
    "                print(\"  Skipping vLLM model - vLLM not installed.\")\n",
    "                continue\n",
    "            # VLLMInference now handles internal parsing/warnings based on VLLM_DEVICE\n",
    "            # and uses VLLM_TENSOR_PARALLEL_SIZE. It relies on external CUDA_VISIBLE_DEVICES.\n",
    "            llm_player = VLLMInference(model_config, GENERATION_CONFIG)\n",
    "        elif inference_type == \"openai\":\n",
    "             if not OPENAI_AVAILABLE:\n",
    "                 print(\"  Skipping OpenAI model - OpenAI library not installed.\")\n",
    "                 continue\n",
    "             llm_player = OpenAIInference(model_config, GENERATION_CONFIG)\n",
    "        else:\n",
    "            print(f\"  Skipping model - Unknown inference type: {inference_type}\")\n",
    "            continue\n",
    "        # --- LLM Initialization Complete ---\n",
    "\n",
    "        # --- Optional: Run ONE visualized game first ---\n",
    "        if run_visualization and config_idx == 0: # Only visualize the very first model's first game\n",
    "             print(f\"\\n--- Running ONE visualized game for {model_name} (Style: {VIZ_STYLE.get('font', 'Default')}) ---\")\n",
    "             try:\n",
    "                 # Pass the selected style to play_game\n",
    "                 game_result_viz = play_game(\n",
    "                     llm_player,\n",
    "                     GENERATION_CONFIG,\n",
    "                     visualize=False, # Enable visualization\n",
    "                     viz_delay=VISUALIZATION_DELAY_SECONDS,\n",
    "                     viz_style=VIZ_STYLE # Pass the chosen style dictionary\n",
    "                 )\n",
    "\n",
    "                 print(\"\\n--- Visualization Game Summary ---\")\n",
    "                 print(f\"  Seed: {game_result_viz.get('game_seed', 'N/A')}\")\n",
    "                 print(f\"  Won: {game_result_viz.get('win', 'N/A')}\")\n",
    "                 print(f\"  Score: {game_result_viz.get('final_score', 'N/A')}\")\n",
    "                 print(f\"  Max Tile: {game_result_viz.get('max_tile_achieved', 'N/A')}\")\n",
    "                 print(f\"  Moves: {game_result_viz.get('num_moves', 'N/A')}\")\n",
    "                 if game_result_viz.get('error'): print(f\"  Error: {game_result_viz['error']}\")\n",
    "                 print(\"-\" * 30)\n",
    "                 # Note: This visualized run is NOT logged by default to avoid skewing stats\n",
    "\n",
    "             except Exception as viz_err:\n",
    "                  print(f\"\\n  Error during visualized game run for model {model_name}: {viz_err}\")\n",
    "                  traceback.print_exc()\n",
    "             # input(\"Press Enter to continue with standard evaluation runs...\") # Optional pause\n",
    "        # ----------------------------------------------\n",
    "\n",
    "\n",
    "        # --- Run Standard Evaluation Games ---\n",
    "        run_results = []\n",
    "        print(f\"\\n  Starting {NUM_RUNS} standard evaluation runs (no visualization)...\")\n",
    "        for run_id in tqdm(range(1, NUM_RUNS + 1), desc=f\"Playing ({model_name})\", unit=\"game\"):\n",
    "            try:\n",
    "                # Pass visualize=False to the main evaluation runs\n",
    "                game_result = play_game(llm_player, GENERATION_CONFIG, visualize=False) # Use default random seed per game\n",
    "                game_result[\"run_id\"] = run_id # Add run_id for tracking\n",
    "                # Log generation parameters used for this run\n",
    "                game_result[\"temperature\"] = llm_player.generation_config.get(\"temperature\")\n",
    "                game_result[\"max_new_tokens\"] = llm_player.generation_config.get(\"max_new_tokens\")\n",
    "                game_result[\"top_p\"] = llm_player.generation_config.get(\"top_p\")\n",
    "                game_result[\"min_p\"] = llm_player.generation_config.get(\"min_p\")\n",
    "                log_results(game_result, RESULTS_FILE)\n",
    "                run_results.append(game_result)\n",
    "            except Exception as game_err:\n",
    "                 print(f\"\\n  Error during game run {run_id} for model {model_name}: {game_err}\")\n",
    "                 traceback.print_exc()\n",
    "                 # Log failure with generation params\n",
    "                 error_result = {\n",
    "                     \"timestamp\": datetime.now().isoformat(),\n",
    "                     \"model_name\": model_name,\n",
    "                     \"inference_type\": inference_type,\n",
    "                     \"run_id\": run_id,\n",
    "                     \"game_seed\": None,\n",
    "                     \"max_tile_achieved\": 0,\n",
    "                     \"final_score\": 0,\n",
    "                     \"num_moves\": 0,\n",
    "                     \"win\": False,\n",
    "                     \"duration_seconds\": 0,\n",
    "                     \"temperature\": GENERATION_CONFIG.get(\"temperature\"),\n",
    "                     \"max_new_tokens\": GENERATION_CONFIG.get(\"max_new_tokens\"),\n",
    "                     \"top_p\": GENERATION_CONFIG.get(\"top_p\"),\n",
    "                     \"min_p\": GENERATION_CONFIG.get(\"min_p\"),\n",
    "                     \"error\": f\"Game execution error: {str(game_err)}\"\n",
    "                 }\n",
    "                 log_results(error_result, RESULTS_FILE)\n",
    "                 run_results.append(error_result) # Add error result to list\n",
    "        # --------------------------------------\n",
    "\n",
    "        # --- Calculate Summary Statistics for this Model ---\n",
    "        if run_results:\n",
    "            successful_runs = [r for r in run_results if r.get(\"error\") is None or \"Game execution error\" not in str(r.get(\"error\", \"\"))] # More robust error check\n",
    "            num_successful = len(successful_runs)\n",
    "            if num_successful > 0:\n",
    "                 avg_score = np.mean([r['final_score'] for r in successful_runs])\n",
    "                 std_score = np.std([r['final_score'] for r in successful_runs])\n",
    "                 avg_max_tile = np.mean([r['max_tile_achieved'] for r in successful_runs])\n",
    "                 std_max_tile = np.std([r['max_tile_achieved'] for r in successful_runs])\n",
    "                 win_rate = np.mean([r['win'] for r in successful_runs]) * 100\n",
    "                 avg_moves = np.mean([r['num_moves'] for r in successful_runs])\n",
    "\n",
    "                 model_summary_stats[model_name] = {\n",
    "                     \"avg_score\": avg_score,\n",
    "                     \"std_score\": std_score,\n",
    "                     \"avg_max_tile\": avg_max_tile,\n",
    "                     \"std_max_tile\": std_max_tile,\n",
    "                     \"win_rate_percent\": win_rate,\n",
    "                     \"avg_moves\": avg_moves,\n",
    "                     \"successful_runs\": num_successful,\n",
    "                     \"total_runs\": NUM_RUNS\n",
    "                 }\n",
    "                 print(f\"\\n  Summary for {model_name} ({num_successful}/{NUM_RUNS} successful runs):\")\n",
    "                 print(f\"    Avg Score: {avg_score:.2f}  {std_score:.2f}\")\n",
    "                 print(f\"    Avg Max Tile: {avg_max_tile:.2f}  {std_max_tile:.2f}\")\n",
    "                 print(f\"    Win Rate (>=2048): {win_rate:.1f}%\")\n",
    "                 print(f\"    Avg Moves: {avg_moves:.1f}\")\n",
    "            else:\n",
    "                 print(f\"\\n  No successful standard runs completed for {model_name} to calculate stats.\")\n",
    "                 model_summary_stats[model_name] = {\"successful_runs\": 0, \"total_runs\": NUM_RUNS, \"error\": \"No successful runs\"}\n",
    "        else:\n",
    "            print(f\"\\n  No results recorded for standard runs of {model_name}.\")\n",
    "            model_summary_stats[model_name] = {\"successful_runs\": 0, \"total_runs\": NUM_RUNS, \"error\": \"No standard runs recorded\"}\n",
    "        # ----------------------------------------------------\n",
    "\n",
    "    except Exception as model_init_err:\n",
    "        print(f\"\\n  FATAL ERROR initializing or running model {model_name}: {model_init_err}\")\n",
    "        traceback.print_exc()\n",
    "        model_summary_stats[model_name] = {\"successful_runs\": 0, \"total_runs\": NUM_RUNS, \"error\": f\"Initialization/Fatal Error: {str(model_init_err)}\"}\n",
    "\n",
    "    finally:\n",
    "        # --- Cleanup LLM Resources ---\n",
    "        if llm_player:\n",
    "            print(f\"Cleaning up resources for model: {model_name}\")\n",
    "            llm_player.cleanup()\n",
    "            llm_player = None # Ensure it's cleared before next loop iteration\n",
    "        print(\"-\" * 40) # Separator after model cleanup\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"=== Evaluation Complete ===\")\n",
    "print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "# --- Final Summary ---\n",
    "print(\"Overall Model Summary:\")\n",
    "for name, stats in model_summary_stats.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    if stats.get(\"error\") and stats.get(\"successful_runs\", -1) == 0: # Check if error exists and no successful runs\n",
    "         print(f\"  Status: Failed ({stats.get('error', 'Unknown error')})\")\n",
    "    elif \"successful_runs\" in stats:\n",
    "        print(f\"  Successful Runs: {stats.get('successful_runs', 'N/A')}/{stats.get('total_runs', 'N/A')}\")\n",
    "        # Check if stats were actually calculated before printing\n",
    "        if \"avg_score\" in stats:\n",
    "             print(f\"    Avg Score: {stats.get('avg_score', 'N/A'):.2f}  {stats.get('std_score', 'N/A'):.2f}\")\n",
    "             print(f\"    Avg Max Tile: {stats.get('avg_max_tile', 'N/A'):.2f}  {stats.get('std_max_tile', 'N/A'):.2f}\")\n",
    "             print(f\"    Win Rate (%): {stats.get('win_rate_percent', 'N/A'):.1f}\")\n",
    "             print(f\"    Avg Moves: {stats.get('avg_moves', 'N/A'):.1f}\")\n",
    "        else:\n",
    "             print(\"    (No stats calculated - likely no successful runs)\")\n",
    "    else:\n",
    "        print(\"  Status: Unknown (No stats dictionary populated correctly)\")\n",
    "\n",
    "\n",
    "print(f\"\\nDetailed results logged to: {RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall Model Summary:\")\n",
    "for name, stats in model_summary_stats.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    if \"error\" in stats and stats[\"successful_runs\"] == 0:\n",
    "         print(f\"  Status: Failed ({stats.get('error', 'Unknown error')})\")\n",
    "    else:\n",
    "        print(f\"  Successful Runs: {stats['successful_runs']}/{stats['total_runs']}\")\n",
    "        print(f\"    Avg Score: {stats.get('avg_score', 'N/A'):.2f}\")\n",
    "        print(f\"    Avg Max Tile: {stats.get('avg_max_tile', 'N/A'):.2f}\")\n",
    "        print(f\"    Win Rate (%): {stats.get('win_rate_percent', 'N/A'):.1f}\")\n",
    "        print(f\"    Avg Moves: {stats.get('avg_moves', 'N/A'):.1f}\")\n",
    "\n",
    "\n",
    "print(f\"\\nDetailed results logged to: {RESULTS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Optional: Analysis with Pandas ---\n",
    "# import pandas as pd\n",
    "# try:\n",
    "#     df = pd.read_csv(RESULTS_FILE)\n",
    "#     print(\"\\n--- Results DataFrame Head ---\")\n",
    "#     print(df.head())\n",
    "#     print(\"\\n--- Basic Stats per Model ---\")\n",
    "#     print(df.groupby('model_name')[['max_tile_achieved', 'final_score', 'win', 'duration_seconds']].agg(['mean', 'std', 'count', 'max']))\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"\\nResults file {RESULTS_FILE} not found for analysis.\")\n",
    "# except Exception as pd_err:\n",
    "#      print(f\"\\nError analyzing results with pandas: {pd_err}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
